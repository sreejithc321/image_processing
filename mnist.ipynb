{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sreejith/.local/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(784, input_dim=784, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "/home/sreejith/.local/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, activation=\"softmax\", kernel_initializer=\"normal\")`\n",
      "/home/sreejith/.local/lib/python3.6/site-packages/ipykernel_launcher.py:49: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 9s - loss: 0.2790 - acc: 0.9208 - val_loss: 0.1415 - val_acc: 0.9571\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.1116 - acc: 0.9677 - val_loss: 0.0919 - val_acc: 0.9707\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0718 - acc: 0.9795 - val_loss: 0.0785 - val_acc: 0.9775\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.0503 - acc: 0.9855 - val_loss: 0.0735 - val_acc: 0.9773\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.0373 - acc: 0.9892 - val_loss: 0.0680 - val_acc: 0.9788\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.0267 - acc: 0.9928 - val_loss: 0.0616 - val_acc: 0.9812\n",
      "Epoch 7/10\n",
      " - 8s - loss: 0.0207 - acc: 0.9949 - val_loss: 0.0608 - val_acc: 0.9819\n",
      "Epoch 8/10\n",
      " - 8s - loss: 0.0142 - acc: 0.9967 - val_loss: 0.0627 - val_acc: 0.9800\n",
      "Epoch 9/10\n",
      " - 8s - loss: 0.0109 - acc: 0.9976 - val_loss: 0.0595 - val_acc: 0.9809\n",
      "Epoch 10/10\n",
      " - 8s - loss: 0.0079 - acc: 0.9986 - val_loss: 0.0582 - val_acc: 0.9823\n",
      "Baseline Error: 1.77%\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype( 'float32' )\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype( 'float32' )\n",
    "\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "\n",
    "# define baseline model\n",
    "def mlp_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, init= 'normal' , activation= 'relu' ))\n",
    "    model.add(Dense(num_classes, init= 'normal' , activation= 'softmax' ))\n",
    "    # Compile model\n",
    "    model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = mlp_model()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=10, batch_size=200,verbose=2)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sreejith/.local/lib/python3.6/site-packages/ipykernel_launcher.py:46: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), input_shape=(1, 28, 28..., activation=\"relu\", padding=\"valid\")`\n",
      "/home/sreejith/.local/lib/python3.6/site-packages/ipykernel_launcher.py:61: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 238s - loss: 0.2231 - acc: 0.9364 - val_loss: 0.0790 - val_acc: 0.9748\n",
      "Epoch 2/10\n",
      " - 238s - loss: 0.0712 - acc: 0.9784 - val_loss: 0.0454 - val_acc: 0.9841\n",
      "Epoch 3/10\n",
      " - 237s - loss: 0.0511 - acc: 0.9844 - val_loss: 0.0453 - val_acc: 0.9851\n",
      "Epoch 4/10\n",
      " - 237s - loss: 0.0393 - acc: 0.9879 - val_loss: 0.0408 - val_acc: 0.9871\n",
      "Epoch 5/10\n",
      " - 237s - loss: 0.0326 - acc: 0.9897 - val_loss: 0.0345 - val_acc: 0.9886\n",
      "Epoch 6/10\n",
      " - 236s - loss: 0.0266 - acc: 0.9916 - val_loss: 0.0327 - val_acc: 0.9893\n",
      "Epoch 7/10\n",
      " - 238s - loss: 0.0221 - acc: 0.9930 - val_loss: 0.0363 - val_acc: 0.9882\n",
      "Epoch 8/10\n",
      " - 246s - loss: 0.0193 - acc: 0.9940 - val_loss: 0.0335 - val_acc: 0.9887\n",
      "Epoch 9/10\n",
      " - 315s - loss: 0.0158 - acc: 0.9950 - val_loss: 0.0325 - val_acc: 0.9893\n",
      "Epoch 10/10\n",
      " - 238s - loss: 0.0142 - acc: 0.9957 - val_loss: 0.0327 - val_acc: 0.9898\n",
      "CNN Error: 1.02%\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "# reshape to be [samples][channels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype( 'float32' )\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype( 'float32' )\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "# reshape to be [samples][channels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype( 'float32' )\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype( 'float32' )\n",
    "\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "def cnn_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 5, 5, border_mode= 'valid' , input_shape=(1, 28, 28), activation= 'relu' ))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation= 'relu' ))\n",
    "    model.add(Dense(num_classes, activation= 'softmax' ))\n",
    "    # Compile model\n",
    "    model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = cnn_model()\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=10, batch_size=200,verbose=2)\n",
    "\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
